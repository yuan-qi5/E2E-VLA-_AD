# VLA
my learning notes of VLA <br>
- [paper][1]
  - [CoVLA][3] : Comprehensive Vision-Language-Action Dataset for Autonomous Driving (24/08 arxiv)
  - [OpenDriveVLA][4] : OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model (25/03 arxiv)
  - [AlphaDrive][5] : AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning (20/03 arxiv)
  - [DriveLMM-o1][6] : DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding (25/03 arxiv)
  - [Asyncdriver][7] : Asynchronous Large Language Model Enhanced Planner for Autonomous Driving (24/06 arxiv)
  - [LanguageMPC][8] : Large Language Models As Decision Makers For Autonomous Driving (23/10 arxiv)
  - 



- [code][2]


[1]:https://github.com/yuan-qi5/VLA/tree/main/paper
[2]:https://github.com/yuan-qi5/VLA/tree/main/code
[3]:https://github.com/yuan-qi5/VLA/blob/main/paper/CoVLA.md
[4]:https://github.com/yuan-qi5/VLA/blob/main/paper/OpenDriveVLA.md
[5]:https://github.com/yuan-qi5/VLA/blob/main/paper/AlphaDrive.md
[6]:https://github.com/yuan-qi5/VLA/blob/main/paper/DriveLMM-o1.md
[7]:https://github.com/yuan-qi5/VLA/blob/main/paper/Asyncdriver.md
[8]:https://github.com/yuan-qi5/VLA/blob/main/paper/LanguageMPC.md








