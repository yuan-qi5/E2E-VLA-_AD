# VLA
my learning notes of VLA <br>
- [paper][1]
  - [CoVLA][3] : Comprehensive Vision-Language-Action Dataset for Autonomous Driving
  - [OpenDriveVLA][4] : OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model
  - [AlphaDrive][5] : AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning
  - [DriveLMM-o1][6] : DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding
- [code][2]


[1]:https://github.com/yuan-qi5/VLA/tree/main/paper
[2]:https://github.com/yuan-qi5/VLA/tree/main/code
[3]:https://github.com/yuan-qi5/VLA/blob/main/paper/CoVLA.md
[4]:https://github.com/yuan-qi5/VLA/blob/main/paper/OpenDriveVLA.md
[5]:https://github.com/yuan-qi5/VLA/blob/main/paper/AlphaDrive.md
[6]:https://github.com/yuan-qi5/VLA/blob/main/paper/DriveLMM-o1.md

