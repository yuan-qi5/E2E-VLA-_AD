# VLA
my learning notes of VLA <br>
- [paper][1]
  - [CoVLA][3] : Comprehensive Vision-Language-Action Dataset for Autonomous Driving (24/08 arxiv)
  - [OpenDriveVLA][4] : OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model (25/03 arxiv)
  - [AlphaDrive][5] : AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning (25/03 arxiv)
  - [DriveLMM-o1][6] : DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding (25/03 arxiv)
  - [Asyncdriver][7] : Asynchronous Large Language Model Enhanced Planner for Autonomous Driving (24/06 arxiv ECCV)
  - [LanguageMPC][8] : Large Language Models As Decision Makers For Autonomous Driving (23/10 arxiv )
  - [GameFormer][9] : Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving (23/03 arxiv ICCV)
  - [LightEMMA][10] : Lightweight End-to-End Multimodal  Model for Autonomous Driving
  - [nuPlan][11] : A closed-loop ML-based planning benchmark for autonomous vehicles
  - [nuScenes][12] : A multimodal dataset for autonomous driving

- [code][2]


[1]:https://github.com/yuan-qi5/VLA/tree/main/paper
[2]:https://github.com/yuan-qi5/VLA/tree/main/code
[3]:https://github.com/yuan-qi5/VLA/blob/main/paper/CoVLA.md
[4]:https://github.com/yuan-qi5/VLA/blob/main/paper/OpenDriveVLA.md
[5]:https://github.com/yuan-qi5/VLA/blob/main/paper/AlphaDrive.md
[6]:https://github.com/yuan-qi5/VLA/blob/main/paper/DriveLMM-o1.md
[7]:https://github.com/yuan-qi5/VLA/blob/main/paper/Asyncdriver.md
[8]:https://github.com/yuan-qi5/VLA/blob/main/paper/LanguageMPC.md
[9]:https://github.com/yuan-qi5/VLA/blob/main/paper/GameFormer.md
[10]:https://github.com/yuan-qi5/VLA/blob/main/paper/LightEMMA.md
[11]:https://github.com/yuan-qi5/VLA/blob/main/paper/nuPlan.md
[12]:https://github.com/yuan-qi5/VLA/blob/main/paper/nuScenes.md






